{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "#Load Data\n",
    "def load_raw(path:str,debug:bool=False) -> DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    #\n",
    "    if debug:\n",
    "        print(df.to_string())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "\n",
    "def preprocess_data(df:DataFrame,save_to_file:bool=True,debug:bool=False):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # Function to lemmatize text using spaCy\n",
    "    def lemmatize_text(text):\n",
    "        doc = nlp(text)\n",
    "        return \" \".join([token.lemma_ for token in doc])\n",
    "    data = df\n",
    "\n",
    "    # Extract player names and remove from main data\n",
    "    character_names = data.iloc[:, 0].tolist()\n",
    "    main_data = data.iloc[:, 1:]\n",
    "\n",
    "    # Identify numeric and text columns\n",
    "    numeric_columns = main_data.select_dtypes(include=[np.number]).columns\n",
    "    text_columns = main_data.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    # Preprocess numeric columns\n",
    "    scaler = StandardScaler()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    numeric_data = pd.DataFrame(scaler.fit_transform(imputer.fit_transform(main_data[numeric_columns])), \n",
    "                                columns=numeric_columns)\n",
    "\n",
    "    print(main_data[text_columns].shape)\n",
    "\n",
    "    # Preprocess text columns using TfidfVectorizer and spaCy lemmatization\n",
    "    #tfidf = TfidfVectorizer()  # You can adjust max_features as needed\n",
    "    #text_data = main_data[text_columns].fillna('')\n",
    "    #text_data_combined = text_data.apply(lambda x: ' '.join(x), axis=1)\n",
    "    #lemmatized_text = text_data_combined.apply(lemmatize_text)\n",
    "    #tfidf_matrix = tfidf.fit_transform(lemmatized_text)\n",
    "    #tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "    # Combine processed data # tfidf_df\n",
    "    processed_data = pd.concat([numeric_data], axis=1)\n",
    "    processed_data.insert(loc = 0,\n",
    "          column = \"player\",\n",
    "          value = character_names)\n",
    "    if save_to_file:\n",
    "        processed_data.to_csv(\"processed_data.csv\",index=False)\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def decompose_data(df: pd.DataFrame, method: str = \"PCA\", dimensions: int = 2,save_to_file:bool = True, debug: bool = False):\n",
    "    \"\"\"\n",
    "    Function to apply dimensionality reduction using PCA or t-SNE.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame (includes the first column with names)\n",
    "    - method: The dimensionality reduction method to use (\"PCA\" or \"TSNE\")\n",
    "    - dimensions: Number of dimensions to reduce to (default is 3)\n",
    "    - debug: If True, prints intermediate debugging information\n",
    "    \n",
    "    Returns:\n",
    "    - Transformed DataFrame with reduced dimensions, preserving the first column with names\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract player names and remove from main data\n",
    "    character_names = df.iloc[:, 0].tolist()\n",
    "    main_data = df.iloc[:, 1:]\n",
    "    \n",
    "    # Perform dimensionality reduction based on the method\n",
    "    if method.upper() == \"PCA\":\n",
    "        pca = PCA(n_components=dimensions)\n",
    "        reduced_data = pca.fit_transform(main_data)\n",
    "        \n",
    "        if debug:\n",
    "            explained_variance = pca.explained_variance_ratio_\n",
    "            print(f\"Explained variance ratio by each principal component: {explained_variance}\")\n",
    "            print(f\"Total explained variance: {explained_variance.sum()}\")\n",
    "    \n",
    "    elif method.upper() == \"TSNE\":\n",
    "        tsne = TSNE(n_components=dimensions, random_state=42)\n",
    "        reduced_data = tsne.fit_transform(main_data)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"t-SNE with {dimensions} dimensions applied.\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Method '{method}' is not supported. Choose 'PCA' or 'TSNE'.\")\n",
    "\n",
    "    # Combine character names with the reduced data\n",
    "    reduced_df = pd.DataFrame(reduced_data, columns=[f\"Dimension_{i+1}\" for i in range(dimensions)])\n",
    "    reduced_df.insert(0, 'Name', character_names)\n",
    "    if save_to_file:\n",
    "        reduced_df.to_csv(\"reduced_df.csv\",index=False)\n",
    "    if debug:\n",
    "        print(\"Decomposition completed.\")\n",
    "    \n",
    "    return reduced_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def cluster_and_plot(df: pd.DataFrame, clusters: int = 3, dimensions: int = 2, debug: bool = False):\n",
    "    \"\"\"\n",
    "    Function to perform clustering on the reduced data and plot the clusters.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The input DataFrame (assumed to be the output from the decompose_data function).\n",
    "    - clusters: Number of clusters for KMeans (default is 3).\n",
    "    - dimensions: Number of dimensions for plotting (2 or 3).\n",
    "    - debug: If True, prints additional information.\n",
    "    \n",
    "    Returns:\n",
    "    - None, but plots the clustered data.\n",
    "    \"\"\"\n",
    "    \n",
    "    if dimensions not in [2, 3]:\n",
    "        raise ValueError(\"Plotting only supports 2 or 3 dimensions.\")\n",
    "    \n",
    "    # Extract player names and reduced data for clustering\n",
    "    character_names = df['Name'].tolist()\n",
    "    main_data = df.iloc[:, 1:].values  # Everything but the 'Name' column\n",
    "\n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=clusters, random_state=42)\n",
    "    df['Cluster'] = kmeans.fit_predict(main_data)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Cluster centers: {kmeans.cluster_centers_}\")\n",
    "        print(f\"Labels assigned to data points: {kmeans.labels_}\")\n",
    "    \n",
    "    # 2D or 3D plot based on the 'dimensions' parameter\n",
    "    if dimensions == 2:\n",
    "        plt.figure(figsize=(10, 7),dpi=300)\n",
    "        plt.scatter(df['Dimension_1'], df['Dimension_2'], c=df['Cluster'], cmap='viridis', s=50, alpha=0.7)\n",
    "        \n",
    "        # Annotate each point with its character name\n",
    "        for i, name in enumerate(character_names):\n",
    "            plt.text(df['Dimension_1'][i], df['Dimension_2'][i], name, fontsize=5, ha='right')\n",
    "\n",
    "        plt.title(f'2D Clustering Plot with {clusters} Clusters')\n",
    "        plt.xlabel('Dimension 1')\n",
    "        plt.ylabel('Dimension 2')\n",
    "        #plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    elif dimensions == 3:\n",
    "        fig = plt.figure(figsize=(10, 7),dpi=300)\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        scatter = ax.scatter(df['Dimension_1'], df['Dimension_2'], df['Dimension_3'], c=df['Cluster'], cmap='viridis', s=50, alpha=0.7)\n",
    "        \n",
    "        # Annotate each point with its character name\n",
    "        for i, name in enumerate(character_names):\n",
    "            ax.text(df['Dimension_1'][i], df['Dimension_2'][i], df['Dimension_3'][i], name, fontsize=5)\n",
    "\n",
    "        ax.set_title(f'3D Clustering Plot with {clusters} Clusters')\n",
    "        ax.set_xlabel('Dimension 1')\n",
    "        ax.set_ylabel('Dimension 2')\n",
    "        ax.set_zlabel('Dimension 3')\n",
    "        #fig.legends(scatter, ax=ax, label='Cluster')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = 3\n",
    "clusters = 6\n",
    "df = load_raw(\"raw_data.csv\")\n",
    "df = preprocess_data(df)\n",
    "df = decompose_data(df,\"PCA\", dims)\n",
    "cluster_and_plot(df,clusters,2)\n",
    "if dims == 3:\n",
    "    cluster_and_plot(df,clusters,dims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
